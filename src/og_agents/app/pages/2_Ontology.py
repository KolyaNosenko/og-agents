import streamlit as st
from dataclasses import dataclass
from collections import defaultdict
from langgraph.graph.state import CompiledStateGraph
from rdflib import Graph
import streamlit.components.v1 as components

from og_agents.config import AppConfig
from og_agents.documents import OntologySourceDocument
from og_agents.common.http_client import RequestsHttpClient
from og_agents.ontology.ontology_storage import OntologyStorage
from og_agents.workflows import OntologyGenerationWorkflowBuilder, WorkflowContext
from og_agents.workflows.nodes import (
    GENERATE_COMPETENCY_QUESTIONS_NODE_NAME,
    GENERATE_ONTOLOGY_NODE_NAME,
    ONTOLOGY_CONSISTENCY_VALIDATION_NODE_NAME,
    ONTOLOGY_RDF_SYNTAX_VALIDATION_NODE_NAME,
    OOPS_ONTOLOGY_VALIDATION_NODE_NAME,
)
from og_agents.workflows.requests import GenerateOntologyRequest
from og_agents.language_models import OpenAILanguageModel
from og_agents.state import GenerationState
from og_agents.app.ontology_visualization import visualize_ontology


@dataclass(frozen=True)
class AppDependencies:
    app_config: AppConfig
    ontology_storage: OntologyStorage
    http_client: RequestsHttpClient
    language_model: OpenAILanguageModel
    ontology_generation_workflow: CompiledStateGraph[GenerationState, WorkflowContext, GenerateOntologyRequest]


@st.cache_resource
def init_app_dependencies() -> AppDependencies:
    app_config = AppConfig.init()
    storage = OntologyStorage(app_config)
    http_client = RequestsHttpClient()
    language_model = OpenAILanguageModel.create(app_config)  # streaming must be enabled inside this wrapper
    og_generation_workflow = OntologyGenerationWorkflowBuilder.create(app_config).build()

    return AppDependencies(
        app_config=app_config,
        ontology_storage=storage,
        http_client=http_client,
        language_model=language_model,
        ontology_generation_workflow=og_generation_workflow,
    )


st.set_page_config(layout="wide")

app_dependencies = init_app_dependencies()
ontology_storage = app_dependencies.ontology_storage
ontology_generation_workflow = app_dependencies.ontology_generation_workflow

if "is_ontology_exists" not in st.session_state:
    st.session_state.is_ontology_exists = False

INITIAL_LIMIT = 1000

try:
    is_ontology_exists = ontology_storage.is_exist()
    st.session_state.is_ontology_exists = is_ontology_exists
except Exception as e:
    st.error(f"–ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –æ–Ω—Ç–æ–ª–æ–≥—ñ—ó: {e}")

if not st.session_state.is_ontology_exists:
    st.title("–û–Ω—Ç–æ–ª–æ–≥—ñ—é –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ, —Å—Ç–≤–æ—Ä—ñ—Ç—å –Ω–æ–≤—É")

    uploaded_file = st.file_uploader("–î–æ–¥–∞—Ç–∏ —Ñ–∞–π–ª", type=["txt"])

    if uploaded_file is not None:
        text = uploaded_file.read().decode("utf-8", errors="ignore")

        st.header("–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –æ–Ω—Ç–æ–ª–æ–≥—ñ—ó –∑ —Ç–µ–∫—Å—Ç—É:")
        st.caption(text)

        workflow_execution = st.container()
        last_step = None

        llm_placeholders = {}              # step -> st.empty() (created ONLY in custom branch, at the end)
        llm_buffers = defaultdict(str)     # step -> accumulated tokens

        for evt in ontology_generation_workflow.stream(
            {"documents": [OntologySourceDocument(page_content=text)]},
            context=WorkflowContext(
                config=app_dependencies.app_config,
                http_client=app_dependencies.http_client,
                language_model=app_dependencies.language_model,
                ontology_storage=app_dependencies.ontology_storage,
            ),
            stream_mode=["custom", "messages"],
        ):
            with workflow_execution:
                mode = None
                payload = evt
                if isinstance(evt, tuple):
                    if len(evt) == 3:
                        _namespace, mode, payload = evt
                    elif len(evt) == 2:
                        a, b = evt
                        if isinstance(a, str) and a in ("custom", "messages", "updates", "values", "debug"):
                            mode, payload = a, b
                        else:
                            mode, payload = None, evt

                # ----------------------------
                # 1) LLM token streaming: BUFFER ONLY (never create UI here)
                # ----------------------------
                if mode == "messages":
                    try:
                        msg, metadata = payload
                    except Exception:
                        continue

                    step = metadata.get("langgraph_node") or metadata.get("node") or "LLM"
                    delta = getattr(msg, "content", "") or ""
                    if not delta:
                        continue

                    llm_buffers[step] += delta

                    # Render only if placeholder already exists (created in custom branch)
                    if step in llm_placeholders:
                        llm_placeholders[step].code(llm_buffers[step])

                    continue

                # ----------------------------
                # 2) Your custom stream logic (unchanged `message` semantics)
                # ----------------------------
                chunk = payload
                if not isinstance(chunk, dict) or "current_step" not in chunk:
                    continue

                step = chunk["current_step"]

                if not last_step:
                    st.subheader(f"–ü–æ—Ç–æ—á–Ω–∏–π –∫—Ä–æ–∫ {step}", divider=True)
                    last_step = step
                elif last_step != step:
                    st.subheader(f"–ü–æ—Ç–æ—á–Ω–∏–π –∫—Ä–æ–∫ {step}", divider=True)
                    last_step = step

                # ---- your existing rendering (all your writes happen BEFORE LLM placeholder) ----
                if step == GENERATE_COMPETENCY_QUESTIONS_NODE_NAME:
                    st.write(chunk["message"])

                elif step == GENERATE_ONTOLOGY_NODE_NAME:
                    st.write(chunk["message"])

                elif step == ONTOLOGY_RDF_SYNTAX_VALIDATION_NODE_NAME:
                    if "error" in chunk:
                        st.write(chunk["message"])
                        st.warning(chunk["error"])
                    else:
                        st.write(chunk["message"])

                elif step == OOPS_ONTOLOGY_VALIDATION_NODE_NAME:
                    if "error" in chunk:
                        st.write(chunk["message"])
                        st.warning(chunk["error"])
                    else:
                        st.write(chunk["message"])

                elif step == ONTOLOGY_CONSISTENCY_VALIDATION_NODE_NAME:
                    if "error" in chunk:
                        st.write(chunk["message"])
                        st.warning(chunk["error"])
                    elif "ontology_ttl" in chunk:
                        st.write(chunk["message"])
                        # st.code(chunk["ontology_ttl"])
                    else:
                        st.write(chunk["message"])

                else:
                    st.write(chunk.get("message", ""))

                if step not in llm_placeholders:
                    llm_placeholders[step] = st.empty()

                # If tokens already streamed, show them now (still at the end)
                if llm_buffers.get(step):
                    llm_placeholders[step].code(llm_buffers[step])

        else:
            with workflow_execution:
                st.success("–û–Ω—Ç–æ–ª–æ–≥—ñ—é —Å—Ç–≤–æ—Ä–µ–Ω–æ!")
                st.divider()
                st.balloons()
                show = st.button("üöÄ –ü–æ–∫–∞–∑–∞—Ç–∏ –æ–Ω—Ç–æ–ª–æ–≥—ñ—é")

                if show:
                    st.session_state.is_ontology_exists = True
                    st.rerun()


@st.dialog("RDF (Turtle)")
def rdf_modal(rdf_graph: Graph):
    ttl_text = rdf_graph.serialize(format="turtle")

    st.download_button(
        "‚¨áÔ∏è –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ ontology.ttl",
        data=ttl_text,
        file_name="ontology.ttl",
        mime="text/turtle",
        use_container_width=True,
    )
    st.code(ttl_text, language="turtle")


if st.session_state.is_ontology_exists:
    st.title("–û–Ω—Ç–æ–ª–æ–≥—ñ—è")

    with st.sidebar:
        st.header("–ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –≥—Ä–∞—Ñ–∞")

        height = st.slider("–í–∏—Å–æ—Ç–∞ (px)", 200, 2000, 700, 50)
        limit = st.slider("–ú–∞–∫—Å. –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑–≤'—è–∑–∫—ñ–≤", 10, 2000, INITIAL_LIMIT, 10)
        initial_zoom = st.slider("–ü–æ—á–∞—Ç–∫–æ–≤–∏–π –∑—É–º", 0.1, 1.0, 0.8, 0.1)

        st.divider()

        show_schema_only = st.checkbox("üìê –¢—ñ–ª—å–∫–∏ —Å—Ö–µ–º–∞ (OWL/RDFS)", value=False)
        show_classes = st.checkbox("–ö–ª–∞—Å–∏", True)
        show_object_props = st.checkbox("Object Properties", True)
        show_data_props = st.checkbox("Datatype Properties", True)
        show_individuals = st.checkbox("–Ü–Ω–¥–∏–≤—ñ–¥–∏", True)

        st.divider()

        show_labels = st.checkbox("üè∑ –ü–æ–∫–∞–∑—É–≤–∞—Ç–∏ –Ω–∞–∑–≤–∏", False)
        show_comments = st.checkbox("üí¨ –ü–æ–∫–∞–∑—É–≤–∞—Ç–∏ –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ", False)
        physics = st.checkbox("üß≤ Physics (layout)", True)

        st.divider()

        reset = st.button("üß® –í–∏–¥–∞–ª–∏—Ç–∏ –û–Ω—Ç–æ–ª–æ–≥—ñ—é")
        if reset:
            ontology_storage.destroy()
            st.cache_data.clear()
            st.success("–û–Ω—Ç–æ–ª–æ–≥—ñ—é –≤–∏–¥–∞–ª–µ–Ω–æ")
            st.session_state.is_ontology_exists = False
            st.rerun()

    try:
        rdf_graph: Graph = ontology_storage.get_world_as_rdf_graph()

        show_rdf = st.button("üìÑ –ü–æ–∫–∞–∑–∞—Ç–∏ RDF (Turtle)")
        if show_rdf:
            rdf_modal(rdf_graph)

        net = visualize_ontology(
            rdf_graph,
            height=height,
            limit=limit,
            show_classes=show_classes,
            show_object_props=show_object_props,
            show_data_props=show_data_props,
            show_individuals=show_individuals,
            show_schema_only=show_schema_only,
            show_labels=show_labels,
            show_comments=show_comments,
            physics=physics,
            initial_zoom=initial_zoom,
        )

        html = net.generate_html()
        components.html(html, height=height + 50, scrolling=True)
    except Exception as e:
        st.error(f"–ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –≥—Ä–∞—Ñ–∞: {e}")
